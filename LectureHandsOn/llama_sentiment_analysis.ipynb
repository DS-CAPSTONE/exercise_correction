{
 "cells": [
  {
   "cell_type": "code",
   "id": "f946e147-304a-40d3-accd-be1d49975a26",
   "metadata": {
    "id": "f946e147-304a-40d3-accd-be1d49975a26"
   },
   "source": [
    "# Let's train the llama sequence classification model to determine whether we are doing the correct exercise or not.\n",
    "# given the sequence he should be able to classify into two things."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The aim was simple - Get the voice, convert it to text, give it to llama to make sequence classification. The classification is done nicely by the llama. We need to train them for the pain and non pain text classification. \n",
    "\n",
    "# If the pain is detected in the voice, then we will ask llama to suggest new exercise for the people. "
   ],
   "id": "49ad92d98f51e33c"
  },
  {
   "cell_type": "code",
   "id": "228ee503-52d8-4b60-ab12-8053008c07de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "228ee503-52d8-4b60-ab12-8053008c07de",
    "outputId": "514ffbec-98a5-4952-8573-3285901734e7"
   },
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install transformers accelerate trl bitsandbytes datasets evaluate huggingface-cli\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install lora\n",
    "# !{sys.executable} -m pip install peft\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install huggingface_hub datasets\n",
    "!{sys.executable} -m pip install huggingface --upgrade\n",
    "!{sys.executable} -m pip install 'accelerate>=0.26.0'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae69018d-4c35-4541-8ff4-32aefddb13a8",
   "metadata": {
    "id": "ae69018d-4c35-4541-8ff4-32aefddb13a8",
    "ExecuteTime": {
     "end_time": "2024-10-21T23:28:34.365675Z",
     "start_time": "2024-10-21T23:28:29.403575Z"
    }
   },
   "source": [
    "from huggingface_hub import login as hlogin\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "hlogin(hf_token)\n",
    "\n",
    "wb_token = \"076d04271de0b9efeb853f25df3d1e4e1b0090a1\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "65d3a3fd-f00a-428e-8f22-3630aa9ce712",
   "metadata": {
    "id": "65d3a3fd-f00a-428e-8f22-3630aa9ce712",
    "ExecuteTime": {
     "end_time": "2024-10-21T23:28:38.897052Z",
     "start_time": "2024-10-21T23:28:37.096813Z"
    }
   },
   "source": [
    "# Load the LLaMA model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Assign a padding token to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model directly onto multiple GPUs using device_map\n",
    "device_map = \"cpu\"  # Automatically balance the model across available GPUs\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    device_map=device_map\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d932c145-4baf-4a3f-846e-bfdb370eafcc",
   "metadata": {
    "id": "d932c145-4baf-4a3f-846e-bfdb370eafcc",
    "ExecuteTime": {
     "end_time": "2024-10-21T23:28:40.467265Z",
     "start_time": "2024-10-21T23:28:40.409386Z"
    }
   },
   "source": [
    "# Paths for Non-Bias and Bias text files\n",
    "non_pain_path = '/Users/jainilpatel/PycharmProjects/Exercise-Correction/LectureHandsOn/not_pain_text'\n",
    "pain_path = '/Users/jainilpatel/PycharmProjects/Exercise-Correction/LectureHandsOn/pain_text'\n",
    "\n",
    "# Make sure the model uses the same padding token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Function to load text files and create a dataset\n",
    "def load_texts_and_labels(non_pain_path, pain_path):\n",
    "    non_bias_texts = []\n",
    "    bias_texts = []\n",
    "\n",
    "    # Load non-bias texts\n",
    "    for file_name in os.listdir(non_pain_path):\n",
    "        with open(os.path.join(non_pain_path, file_name), 'r', encoding='utf-8') as file:\n",
    "            non_bias_texts.append(file.read())\n",
    "\n",
    "    # Load bias texts\n",
    "    # Load bias texts line by line\n",
    "    for file_name in os.listdir(pain_path):\n",
    "        file_path = os.path.join(pain_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                bias_texts.append(line.strip())  # Appending each line to bias_texts\n",
    "\n",
    "    # Create labels\n",
    "    texts = non_bias_texts + bias_texts\n",
    "    labels = [0] * len(non_bias_texts) + [1] * len(bias_texts)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Load data\n",
    "texts, labels = load_texts_and_labels(non_pain_path, pain_path)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': train_labels\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask'],\n",
    "    'labels': val_labels\n",
    "})\n",
    "\n",
    "# Define training arguments with reduced batch size, gradient accumulation, and mixed precision\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    # per_device_train_batch_size=2,  # Reduce batch size\n",
    "    # per_device_eval_batch_size=2,   # Reduce batch size\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    bf16=False,  # Disable bf16 precision\n",
    "    fp16=False,  # Disable fp16 precision\n",
    "    use_cpu=True,  \n",
    "    # gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
    "  \n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the distributed model and datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:28:55.599922Z",
     "start_time": "2024-10-21T23:28:55.594948Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset",
   "id": "1081d871c2964c78",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:30:14.830246Z",
     "start_time": "2024-10-21T23:28:58.040696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.train()\n"
   ],
   "id": "f6f3875ebd896e02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.708990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.414573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.722415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.377924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSafetensorError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/trainer.py:2052\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2050\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2052\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2054\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2056\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2057\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/trainer.py:2467\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2464\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m steps_skipped) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[1;32m   2465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m-> 2467\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2468\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2469\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_substep_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/trainer.py:2918\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2915\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluate(trial, ignore_keys_for_eval)\n\u001B[1;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[0;32m-> 2918\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_save(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/trainer.py:3008\u001B[0m, in \u001B[0;36mTrainer._save_checkpoint\u001B[0;34m(self, model, trial, metrics)\u001B[0m\n\u001B[1;32m   3006\u001B[0m run_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_output_dir(trial\u001B[38;5;241m=\u001B[39mtrial)\n\u001B[1;32m   3007\u001B[0m output_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(run_dir, checkpoint_folder)\n\u001B[0;32m-> 3008\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_internal_call\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3010\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msave_only_model:\n\u001B[1;32m   3011\u001B[0m     \u001B[38;5;66;03m# Save optimizer and scheduler\u001B[39;00m\n\u001B[1;32m   3012\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/trainer.py:3623\u001B[0m, in \u001B[0;36mTrainer.save_model\u001B[0;34m(self, output_dir, _internal_call)\u001B[0m\n\u001B[1;32m   3620\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped\u001B[38;5;241m.\u001B[39msave_checkpoint(output_dir)\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3625\u001B[0m \u001B[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001B[39;00m\n\u001B[1;32m   3626\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpush_to_hub \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _internal_call:\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/trainer.py:3727\u001B[0m, in \u001B[0;36mTrainer._save\u001B[0;34m(self, output_dir, state_dict)\u001B[0m\n\u001B[1;32m   3725\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(state_dict, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_dir, WEIGHTS_NAME))\n\u001B[1;32m   3726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3727\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3728\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe_serialization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_safetensors\u001B[49m\n\u001B[1;32m   3729\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39msave_pretrained(output_dir)\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py:2830\u001B[0m, in \u001B[0;36mPreTrainedModel.save_pretrained\u001B[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001B[0m\n\u001B[1;32m   2825\u001B[0m     gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m   2827\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m safe_serialization:\n\u001B[1;32m   2828\u001B[0m     \u001B[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001B[39;00m\n\u001B[1;32m   2829\u001B[0m     \u001B[38;5;66;03m# joyfulness), but for now this enough.\u001B[39;00m\n\u001B[0;32m-> 2830\u001B[0m     \u001B[43msafe_save_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mformat\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2831\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2832\u001B[0m     save_function(shard, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_directory, shard_file))\n",
      "File \u001B[0;32m~/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/safetensors/torch.py:286\u001B[0m, in \u001B[0;36msave_file\u001B[0;34m(tensors, filename, metadata)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msave_file\u001B[39m(\n\u001B[1;32m    256\u001B[0m     tensors: Dict[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[1;32m    257\u001B[0m     filename: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[1;32m    258\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    259\u001B[0m ):\n\u001B[1;32m    260\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;124;03m    ```\u001B[39;00m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 286\u001B[0m     \u001B[43mserialize_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_flatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mSafetensorError\u001B[0m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T23:30:22.150797Z",
     "start_time": "2024-10-21T23:30:21.395371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Function to perform inference on new text inputs\n",
    "def predict(texts):\n",
    "    # Tokenize the texts just like in the training/validation step\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move the encodings to the correct device (CPU in your case)\n",
    "    input_ids = encodings['input_ids'].to(model.device)\n",
    "    attention_mask = encodings['attention_mask'].to(model.device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Get model predictions (logits)\n",
    "    with torch.no_grad():  # No need to calculate gradients during inference\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Get the predicted label (0 for non-pain, 1 for pain) from logits\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    return predictions.cpu().numpy()  # Return predictions as a NumPy array\n",
    "\n",
    "# Example texts for inference\n",
    "example_texts = [\n",
    "    \"My knee hurts when I bend it.\",\n",
    "    \"I feel perfectly fine after the workout.\"\n",
    "]\n",
    "\n",
    "# Get predictions for new texts\n",
    "predicted_labels = predict(example_texts)\n",
    "\n",
    "# Output the predicted labels\n",
    "for text, label in zip(example_texts, predicted_labels):\n",
    "    print(f\"Text: {text} -> Predicted Label: {label}\")\n"
   ],
   "id": "8e3181e3fe93ef13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My knee hurts when I bend it. -> Predicted Label: 0\n",
      "Text: I feel perfectly fine after the workout. -> Predicted Label: 0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the model. \n",
    "model.save_pretrained('./Trained_Models/pain_model')\n",
    "tokenizer.save_pretrained('./Trained_Models/pain_model')"
   ],
   "id": "43c5cb144c0e116c"
  },
  {
   "cell_type": "code",
   "id": "fc51c911-1f81-46db-b163-a450930455e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc51c911-1f81-46db-b163-a450930455e6",
    "outputId": "bdcc71c4-76d6-4684-970f-52ebbabaf3e5",
    "ExecuteTime": {
     "end_time": "2024-10-21T23:30:56.191309Z",
     "start_time": "2024-10-21T23:30:56.189418Z"
    }
   },
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# Evaluate function\n",
    "def evaluate_model(trainer, val_dataset):\n",
    "    # Get predictions and labels\n",
    "    predictions, labels, _ = trainer.predict(val_dataset)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1)\n",
    "\n",
    "    # Get predicted classes and positive class probabilities\n",
    "    preds = torch.argmax(probabilities, dim=1).numpy()\n",
    "    prob_pain = probabilities[:, 1].numpy()  # Assuming class 1 is 'pain'\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    auc = roc_auc_score(labels, prob_pain)\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1, auc = evaluate_model(trainer, val_dataset)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'AUC: {auc:.4f}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "## how about loading the bert model too and merging the predictions based on the ensemble learning to see whether the f1 score increases or not.",
   "metadata": {
    "id": "ll97AQx657TE"
   },
   "id": "ll97AQx657TE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Ensure both models use the same padding token\n",
    "bert_model.config.pad_token_id = tokenizer.pad_token_id\n"
   ],
   "metadata": {
    "id": "cX5Y6EOP6ynx",
    "ExecuteTime": {
     "end_time": "2024-10-21T23:30:39.135295Z",
     "start_time": "2024-10-21T23:30:39.133354Z"
    }
   },
   "id": "cX5Y6EOP6ynx",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "def get_predictions(model, tokenizer, val_dataset):\n",
    "    # Use the Trainer to get predictions\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "    )\n",
    "    predictions, _, _ = trainer.predict(val_dataset)\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(predictions), dim=-1)\n",
    "    return probabilities.numpy()  # Return as numpy array\n"
   ],
   "metadata": {
    "id": "T4Y5f2Ke6zbe"
   },
   "id": "T4Y5f2Ke6zbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Get predictions from both models\n",
    "llama_probabilities = get_predictions(model, tokenizer, val_dataset)\n",
    "bert_probabilities = get_predictions(bert_model, bert_tokenizer, val_dataset)\n",
    "\n",
    "# Combine predictions (e.g., averaging probabilities)\n",
    "ensemble_probabilities = (llama_probabilities + bert_probabilities) / 2  # Simple average\n",
    "\n",
    "# Get final predictions\n",
    "ensemble_preds = np.argmax(ensemble_probabilities, axis=1)\n"
   ],
   "metadata": {
    "id": "KIgVi3cv640N",
    "ExecuteTime": {
     "end_time": "2024-10-21T23:30:45.801765Z",
     "start_time": "2024-10-21T23:30:45.799793Z"
    }
   },
   "id": "KIgVi3cv640N",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# Get true labels from the validation dataset\n",
    "true_labels = val_dataset['labels'].numpy()  # Adjust based on how your dataset is structured\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, ensemble_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, ensemble_preds, average='binary')\n",
    "auc = roc_auc_score(true_labels, ensemble_probabilities[:, 1])  # Assuming class 1 is 'pain'\n",
    "\n",
    "# Print the ensemble metrics\n",
    "print(f'Ensemble Accuracy: {accuracy:.4f}')\n",
    "print(f'Ensemble Precision: {precision:.4f}')\n",
    "print(f'Ensemble Recall: {recall:.4f}')\n",
    "print(f'Ensemble F1 Score: {f1:.4f}')\n",
    "print(f'Ensemble AUC: {auc:.4f}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWBU-Kaj661W",
    "outputId": "985b8671-085e-4022-b950-194cfdf9ca92"
   },
   "id": "lWBU-Kaj661W",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "EHZ7v_Dn7gwx"
   },
   "id": "EHZ7v_Dn7gwx",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
