{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Detection with the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:03:02.667166Z",
     "start_time": "2024-09-12T04:03:02.499752Z"
    }
   },
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Drawing helpers\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[46134]: Class CaptureDelegate is implemented in both /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x16729c860) and /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x3317ca480). One of the two will be used. Which one is undefined.\n",
      "objc[46134]: Class CVWindow is implemented in both /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x166fe4a68) and /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x3317ca4d0). One of the two will be used. Which one is undefined.\n",
      "objc[46134]: Class CVView is implemented in both /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x166fe4a90) and /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x3317ca4f8). One of the two will be used. Which one is undefined.\n",
      "objc[46134]: Class CVSlider is implemented in both /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x166fe4ab8) and /Users/jainilpatel/PycharmProjects/Exercise-Correction/.venv/lib/python3.9/site-packages/cv2/cv2.abi3.so (0x3317ca520). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the input structure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:03:02.670230Z",
     "start_time": "2024-09-12T04:03:02.667952Z"
    }
   },
   "source": [
    "# Determine important landmarks for plank\n",
    "IMPORTANT_LMS = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_ELBOW\",\n",
    "    \"RIGHT_ELBOW\",\n",
    "    \"LEFT_WRIST\",\n",
    "    \"RIGHT_WRIST\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\",\n",
    "    \"LEFT_HEEL\",\n",
    "    \"RIGHT_HEEL\",\n",
    "    \"LEFT_FOOT_INDEX\",\n",
    "    \"RIGHT_FOOT_INDEX\",\n",
    "]\n",
    "\n",
    "# Generate all columns of the data frame\n",
    "\n",
    "HEADERS = [\"label\"] # Label column\n",
    "\n",
    "for lm in IMPORTANT_LMS:\n",
    "    HEADERS += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup some important functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:03:03.517843Z",
     "start_time": "2024-09-12T04:03:03.514203Z"
    }
   },
   "source": [
    "def extract_important_keypoints(results) -> list:\n",
    "    '''\n",
    "    Extract important keypoints from mediapipe pose detection\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "    data = []\n",
    "    for lm in IMPORTANT_LMS:\n",
    "        keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "        data.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "    \n",
    "    return np.array(data).flatten().tolist()\n",
    "\n",
    "\n",
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:03:17.693222Z",
     "start_time": "2024-09-12T04:03:17.690018Z"
    }
   },
   "source": [
    "# VIDEO_PATH1 = \"../data/plank/plank_test.mov\"\n",
    "# VIDEO_PATH2 = \"../data/plank/plank_test_1.mp4\"\n",
    "# VIDEO_PATH3 = \"../data/plank/plank_test_2.mp4\"\n",
    "# VIDEO_PATH4 = \"../data/plank/plank_test_3.mp4\"\n",
    "# VIDEO_PATH5 = \"../data/plank/plank_test_4.mp4\"\n",
    "VIDEO_TEST = \"../../demo/plank_demo.mp4\"\n",
    "VIDEO_TEST = 0"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make detection with Scikit learn model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:03:20.518570Z",
     "start_time": "2024-09-12T04:03:20.398793Z"
    }
   },
   "source": [
    "# Load model\n",
    "with open(\"./model/LR_model.pkl\", \"rb\") as f:\n",
    "    sklearn_model = pickle.load(f)\n",
    "\n",
    "# Load input scaler\n",
    "with open(\"./model/input_scaler.pkl\", \"rb\") as f2:\n",
    "    input_scaler = pickle.load(f2)\n",
    "\n",
    "# Transform prediction into class\n",
    "def get_class(prediction: float) -> str:\n",
    "    return {\n",
    "        0: \"C\",\n",
    "        1: \"H\",\n",
    "        2: \"L\",\n",
    "    }.get(prediction)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:04:10.807616Z",
     "start_time": "2024-09-12T04:03:22.347055Z"
    }
   },
   "source": [
    "cap = cv2.VideoCapture(VIDEO_TEST)\n",
    "current_stage = \"\"\n",
    "prediction_probability_threshold = 0.6\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 50)\n",
    "        # image = cv2.flip(image, 1)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No human found\")\n",
    "            continue\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "\n",
    "        # Make detection\n",
    "        try:\n",
    "            # Extract keypoints from frame for the input\n",
    "            row = extract_important_keypoints(results)\n",
    "            X = pd.DataFrame([row], columns=HEADERS[1:])\n",
    "            X = pd.DataFrame(input_scaler.transform(X))\n",
    "\n",
    "            # Make prediction and its probability\n",
    "            predicted_class = sklearn_model.predict(X)[0]\n",
    "            predicted_class = get_class(predicted_class)\n",
    "            prediction_probability = sklearn_model.predict_proba(X)[0]\n",
    "            # print(predicted_class, prediction_probability)\n",
    "\n",
    "            # Evaluate model prediction\n",
    "            if predicted_class == \"C\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold:\n",
    "                current_stage = \"Correct\"\n",
    "            elif predicted_class == \"L\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold: \n",
    "                current_stage = \"Low back\"\n",
    "            elif predicted_class == \"H\" and prediction_probability[prediction_probability.argmax()] >= prediction_probability_threshold: \n",
    "                current_stage = \"High back\"\n",
    "            else:\n",
    "                current_stage = \"unk\"\n",
    "            \n",
    "            # Visualization\n",
    "            # Status box\n",
    "            cv2.rectangle(image, (0, 0), (250, 60), (245, 117, 16), -1)\n",
    "\n",
    "            # Display class\n",
    "            cv2.putText(image, \"CLASS\", (95, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, current_stage, (90, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display probability\n",
    "            cv2.putText(image, \"PROB\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(prediction_probability[np.argmax(prediction_probability)], 2)), (10, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        cv2.imshow(\"CV2\", image)\n",
    "        \n",
    "        # Press Q to close cv2 window\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n",
    "  "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 23:03:22.633 python[46134:24774072] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n",
      "No human found\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Make detection with Deep Learning Model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:04:24.469934Z",
     "start_time": "2024-09-12T04:04:20.998006Z"
    }
   },
   "source": [
    "# Load model\n",
    "with open(\"./model/plank_dp.pkl\", \"rb\") as f:\n",
    "    deep_learning_model = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T04:04:30.833547Z",
     "start_time": "2024-09-12T04:04:25.164345Z"
    }
   },
   "source": [
    "cap = cv2.VideoCapture(VIDEO_TEST)\n",
    "current_stage = \"\"\n",
    "prediction_probability_threshold = 0.6\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 50)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        if not results.pose_landmarks:\n",
    "            print(\"No human found\")\n",
    "            continue\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=2), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=1))\n",
    "\n",
    "        # Make detection\n",
    "        try:\n",
    "            # Extract keypoints from frame for the input\n",
    "            row = extract_important_keypoints(results)\n",
    "            X = pd.DataFrame([row, ], columns=HEADERS[1:])\n",
    "            X = pd.DataFrame(input_scaler.transform(X))\n",
    "            \n",
    "\n",
    "            # Make prediction and its probability\n",
    "            prediction = deep_learning_model.predict(X)\n",
    "            predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "            prediction_probability = max(prediction.tolist()[0])\n",
    "            # print(X)\n",
    "\n",
    "            # Evaluate model prediction\n",
    "            if predicted_class == 0 and prediction_probability >= prediction_probability_threshold:\n",
    "                current_stage = \"Correct\"\n",
    "            elif predicted_class == 2 and prediction_probability >= prediction_probability_threshold: \n",
    "                current_stage = \"Low back\"\n",
    "            elif predicted_class == 1 and prediction_probability >= prediction_probability_threshold: \n",
    "                current_stage = \"High back\"\n",
    "            else:\n",
    "                current_stage = \"Unknown\"\n",
    "\n",
    "            # Visualization\n",
    "            # Status box\n",
    "            cv2.rectangle(image, (0, 0), (550, 60), (245, 117, 16), -1)\n",
    "\n",
    "            # # Display class\n",
    "            cv2.putText(image, \"DETECTION\", (95, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, current_stage, (90, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # # Display class\n",
    "            cv2.putText(image, \"CLASS\", (350, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(predicted_class), (345, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # # Display probability\n",
    "            cv2.putText(image, \"PROB\", (15, 12), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(prediction_probability, 2)), (10, 40), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        cv2.imshow(\"CV2\", image)\n",
    "        \n",
    "        # Press Q to close cv2 window\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n",
    "  "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 23:04:26.145885: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9260f401923fb5c4108c543a7d176de9733d378b3752e49535ad7c43c2271b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
